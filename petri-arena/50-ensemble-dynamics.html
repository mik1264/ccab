<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>50. Ensemble Dynamics - Petri Arena</title>
    <style>
        body {
            margin: 0;
            padding: 0;
            display: flex;
            justify-content: center;
            align-items: center;
            min-height: 100vh;
            background: linear-gradient(135deg, #0a0e27 0%, #1a0b2e 100%);
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            color: white;
        }
        .container {
            text-align: center;
        }
        canvas {
            border: 2px solid #00f5ff;
            box-shadow: 0 0 20px rgba(0, 245, 255, 0.5);
            background: #000;
        }
        h1 {
            margin-bottom: 10px;
            font-size: 2rem;
            text-shadow: 0 0 10px rgba(0, 245, 255, 0.8);
        }
        .info {
            margin-top: 15px;
            font-size: 0.9rem;
            opacity: 0.8;
            max-width: 700px;
            line-height: 1.6;
        }
        .stats {
            margin-top: 10px;
            display: flex;
            justify-content: center;
            gap: 20px;
            font-size: 0.95rem;
            flex-wrap: wrap;
        }
        .stat {
            padding: 8px 15px;
            background: rgba(0, 245, 255, 0.1);
            border-radius: 5px;
            border: 1px solid rgba(0, 245, 255, 0.3);
        }
        .agent-label {
            display: inline-block;
            width: 15px;
            height: 15px;
            border-radius: 3px;
            margin-right: 5px;
            vertical-align: middle;
        }
        .controls {
            margin-top: 15px;
            display: flex;
            justify-content: center;
            gap: 10px;
        }
        button {
            padding: 8px 20px;
            font-size: 0.9rem;
            background: rgba(0, 245, 255, 0.2);
            border: 1px solid rgba(0, 245, 255, 0.5);
            color: white;
            border-radius: 5px;
            cursor: pointer;
            transition: all 0.3s;
        }
        button:hover {
            background: rgba(0, 245, 255, 0.4);
            box-shadow: 0 0 10px rgba(0, 245, 255, 0.6);
        }
        .legend {
            margin-top: 10px;
            font-size: 0.85rem;
            opacity: 0.7;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>ðŸŽ­ Ensemble Dynamics</h1>
        <canvas id="canvas"></canvas>
        <div class="stats">
            <div class="stat">
                <span class="agent-label" style="background: #00f5ff;"></span>
                Fast Learner: <span id="agent1">0</span>%
            </div>
            <div class="stat">
                <span class="agent-label" style="background: #ff00ff;"></span>
                Balanced: <span id="agent2">0</span>%
            </div>
            <div class="stat">
                <span class="agent-label" style="background: #00ff88;"></span>
                Conservative: <span id="agent3">0</span>%
            </div>
            <div class="stat">Step: <span id="step">0</span></div>
        </div>
        <div class="legend">
            Fast Learner (Ï„=0.3, high learning) | Balanced (Ï„=0.5, medium learning) | Conservative (Ï„=0.8, low learning)
        </div>
        <div class="controls">
            <button onclick="togglePause()">Pause/Resume</button>
            <button onclick="reset()">Reset</button>
            <button onclick="location.href='index.html'">Back to Gallery</button>
        </div>
        <div class="info">
            <strong>Ensemble Learning:</strong> Three agents with different parameter configurations compete together.
            The "Fast Learner" uses aggressive parameters (low Ï„, high learning rate) for quick adaptation but risk
            of instability. The "Balanced" agent uses moderate parameters for stable growth. The "Conservative" agent
            uses defensive parameters (high Ï„, low learning rate) for robust but slow strategies. Watch how different
            configurations lead to distinct ecological niches and competitive advantages in different phases!
        </div>
    </div>

    <script>
        const canvas = document.getElementById('canvas');
        const ctx = canvas.getContext('2d');

        const gridSize = 128;
        const cellSize = 4;
        canvas.width = gridSize * cellSize;
        canvas.height = gridSize * cellSize;

        const numAgents = 3;
        const alpha = 0.4;

        // Different configurations for each agent
        const agentConfigs = [
            { tau: 0.3, learningRate: 0.25, name: 'Fast Learner' },      // Agent 1: aggressive
            { tau: 0.5, learningRate: 0.15, name: 'Balanced' },          // Agent 2: moderate
            { tau: 0.8, learningRate: 0.08, name: 'Conservative' }       // Agent 3: defensive
        ];

        let state = {
            attack: Array(numAgents + 1).fill(null).map(() =>
                Array(gridSize).fill(null).map(() => Array(gridSize).fill(0))
            ),
            defense: Array(numAgents + 1).fill(null).map(() =>
                Array(gridSize).fill(null).map(() => Array(gridSize).fill(0))
            ),
            hidden: Array(numAgents + 1).fill(null).map(() =>
                Array(gridSize).fill(null).map(() => Array(gridSize).fill(0))
            ),
            aliveness: Array(numAgents + 1).fill(null).map(() =>
                Array(gridSize).fill(null).map(() => Array(gridSize).fill(0))
            )
        };

        let step = 0;
        let paused = false;

        const agentColors = [
            [20, 20, 20],      // Background
            [0, 245, 255],     // Agent 1: cyan (fast learner)
            [255, 0, 255],     // Agent 2: magenta (balanced)
            [0, 255, 136]      // Agent 3: green (conservative)
        ];

        function init() {
            // Background
            for (let y = 0; y < gridSize; y++) {
                for (let x = 0; x < gridSize; x++) {
                    state.attack[0][y][x] = Math.random() * 0.2 - 0.1;
                    state.defense[0][y][x] = Math.random() * 0.2 - 0.1;
                    state.hidden[0][y][x] = Math.random() * 0.2 - 0.1;
                    state.aliveness[0][y][x] = 0.3;
                }
            }

            // Agent 1: top-left (Fast Learner)
            const cx1 = gridSize * 0.25, cy1 = gridSize * 0.25, r1 = 12;
            for (let y = 0; y < gridSize; y++) {
                for (let x = 0; x < gridSize; x++) {
                    const dist = Math.sqrt((x - cx1) ** 2 + (y - cy1) ** 2);
                    if (dist < r1) {
                        state.attack[1][y][x] = Math.random() * 0.5 + 0.5;
                        state.defense[1][y][x] = Math.random() * 0.5 + 0.5;
                        state.hidden[1][y][x] = Math.random() * 0.5;
                        state.aliveness[1][y][x] = 1.0;
                    }
                }
            }

            // Agent 2: top-right (Balanced)
            const cx2 = gridSize * 0.75, cy2 = gridSize * 0.25, r2 = 12;
            for (let y = 0; y < gridSize; y++) {
                for (let x = 0; x < gridSize; x++) {
                    const dist = Math.sqrt((x - cx2) ** 2 + (y - cy2) ** 2);
                    if (dist < r2) {
                        state.attack[2][y][x] = Math.random() * 0.5 + 0.5;
                        state.defense[2][y][x] = Math.random() * 0.5 + 0.5;
                        state.hidden[2][y][x] = Math.random() * 0.5;
                        state.aliveness[2][y][x] = 1.0;
                    }
                }
            }

            // Agent 3: bottom-center (Conservative)
            const cx3 = gridSize * 0.5, cy3 = gridSize * 0.75, r3 = 12;
            for (let y = 0; y < gridSize; y++) {
                for (let x = 0; x < gridSize; x++) {
                    const dist = Math.sqrt((x - cx3) ** 2 + (y - cy3) ** 2);
                    if (dist < r3) {
                        state.attack[3][y][x] = Math.random() * 0.5 + 0.5;
                        state.defense[3][y][x] = Math.random() * 0.5 + 0.5;
                        state.hidden[3][y][x] = Math.random() * 0.5;
                        state.aliveness[3][y][x] = 1.0;
                    }
                }
            }

            step = 0;
        }

        function computeStrength(attacker, defender, y, x) {
            const a = state.attack[attacker][y][x];
            const d = state.defense[defender][y][x];
            return a * d;
        }

        function update() {
            const newState = {
                attack: state.attack.map(a => a.map(row => [...row])),
                defense: state.defense.map(d => d.map(row => [...row])),
                hidden: state.hidden.map(h => h.map(row => [...row])),
                aliveness: state.aliveness.map(a => a.map(row => [...row]))
            };

            for (let y = 1; y < gridSize - 1; y++) {
                for (let x = 1; x < gridSize - 1; x++) {
                    const deltas = [];

                    // Background
                    let avgAttack = 0, avgDefense = 0, avgHidden = 0, count = 0;
                    for (let dy = -1; dy <= 1; dy++) {
                        for (let dx = -1; dx <= 1; dx++) {
                            avgAttack += state.attack[0][y + dy][x + dx];
                            avgDefense += state.defense[0][y + dy][x + dx];
                            avgHidden += state.hidden[0][y + dy][x + dx];
                            count++;
                        }
                    }
                    deltas.push({
                        attack: (avgAttack / count - state.attack[0][y][x]) * 0.1,
                        defense: (avgDefense / count - state.defense[0][y][x]) * 0.1,
                        hidden: (avgHidden / count - state.hidden[0][y][x]) * 0.08
                    });

                    // Each agent with its own learning rate
                    for (let agent = 1; agent <= numAgents; agent++) {
                        avgAttack = 0; avgDefense = 0; avgHidden = 0; count = 0;
                        for (let dy = -1; dy <= 1; dy++) {
                            for (let dx = -1; dx <= 1; dx++) {
                                avgAttack += state.attack[agent][y + dy][x + dx];
                                avgDefense += state.defense[agent][y + dy][x + dx];
                                avgHidden += state.hidden[agent][y + dy][x + dx];
                                count++;
                            }
                        }
                        const lr = agentConfigs[agent - 1].learningRate;
                        deltas.push({
                            attack: (avgAttack / count - state.attack[agent][y][x]) * lr,
                            defense: (avgDefense / count - state.defense[agent][y][x]) * lr,
                            hidden: (avgHidden / count - state.hidden[agent][y][x]) * lr * 0.7
                        });
                    }

                    // Competition with per-agent temperature
                    const strengths = [];
                    for (let i = 0; i <= numAgents; i++) {
                        let totalStrength = 0;
                        for (let j = 0; j <= numAgents; j++) {
                            if (i !== j) {
                                totalStrength += computeStrength(i, j, y, x);
                            }
                        }
                        strengths.push(totalStrength + state.aliveness[i][y][x] * 0.8);
                    }

                    // Use average tau for competition resolution
                    const avgTau = (agentConfigs[0].tau + agentConfigs[1].tau + agentConfigs[2].tau) / 3;
                    const maxStrength = Math.max(...strengths);
                    const expStrengths = strengths.map(s => Math.exp((s - maxStrength) / avgTau));
                    const sumExp = expStrengths.reduce((a, b) => a + b, 0);
                    const weights = expStrengths.map(e => e / sumExp);

                    for (let agent = 0; agent <= numAgents; agent++) {
                        const w = weights[agent];
                        newState.attack[agent][y][x] = Math.max(-1, Math.min(1,
                            state.attack[agent][y][x] + deltas[agent].attack * w
                        ));
                        newState.defense[agent][y][x] = Math.max(-1, Math.min(1,
                            state.defense[agent][y][x] + deltas[agent].defense * w
                        ));
                        newState.hidden[agent][y][x] = Math.max(-1, Math.min(1,
                            state.hidden[agent][y][x] + deltas[agent].hidden * w
                        ));
                        newState.aliveness[agent][y][x] = w;
                    }
                }
            }

            state = newState;
            step++;
        }

        function render() {
            const imageData = ctx.createImageData(gridSize, gridSize);

            for (let y = 0; y < gridSize; y++) {
                for (let x = 0; x < gridSize; x++) {
                    let maxAlive = 0;
                    let winner = 0;
                    for (let agent = 0; agent <= numAgents; agent++) {
                        if (state.aliveness[agent][y][x] > maxAlive) {
                            maxAlive = state.aliveness[agent][y][x];
                            winner = agent;
                        }
                    }

                    const idx = (y * gridSize + x) * 4;
                    const color = agentColors[winner];
                    const intensity = maxAlive;
                    imageData.data[idx] = color[0] * intensity;
                    imageData.data[idx + 1] = color[1] * intensity;
                    imageData.data[idx + 2] = color[2] * intensity;
                    imageData.data[idx + 3] = 255;
                }
            }

            ctx.putImageData(imageData, 0, 0);
            ctx.imageSmoothingEnabled = false;
            ctx.drawImage(canvas, 0, 0, gridSize, gridSize, 0, 0, canvas.width, canvas.height);

            updateStats();
        }

        function updateStats() {
            let territories = [0, 0, 0, 0];
            for (let y = 0; y < gridSize; y++) {
                for (let x = 0; x < gridSize; x++) {
                    let maxAlive = 0;
                    let winner = 0;
                    for (let agent = 0; agent <= numAgents; agent++) {
                        if (state.aliveness[agent][y][x] > maxAlive) {
                            maxAlive = state.aliveness[agent][y][x];
                            winner = agent;
                        }
                    }
                    territories[winner]++;
                }
            }

            const total = gridSize * gridSize;
            document.getElementById('agent1').textContent = ((territories[1] / total) * 100).toFixed(1);
            document.getElementById('agent2').textContent = ((territories[2] / total) * 100).toFixed(1);
            document.getElementById('agent3').textContent = ((territories[3] / total) * 100).toFixed(1);
            document.getElementById('step').textContent = step;
        }

        function animate() {
            if (!paused) {
                update();
                render();
            }
            requestAnimationFrame(animate);
        }

        function togglePause() {
            paused = !paused;
        }

        function reset() {
            init();
            render();
        }

        init();
        animate();
    </script>
</body>
</html>
