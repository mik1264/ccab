<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Learning vs Static (No-Learning) - Petri Arena</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            background: linear-gradient(135deg, #1a1a2e 0%, #16213e 100%);
            color: #e0e0e0;
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            min-height: 100vh;
            padding: 20px;
        }

        h1 {
            font-size: 32px;
            color: #667eea;
            margin-bottom: 10px;
            text-align: center;
        }

        .subtitle {
            font-size: 16px;
            color: #b0b0b0;
            margin-bottom: 30px;
            text-align: center;
            max-width: 800px;
        }

        #container {
            display: flex;
            gap: 30px;
            margin-bottom: 20px;
        }

        .panel {
            display: flex;
            flex-direction: column;
            align-items: center;
        }

        .panel h2 {
            font-size: 20px;
            color: #667eea;
            margin-bottom: 10px;
        }

        .panel-subtitle {
            font-size: 12px;
            color: #888;
            margin-bottom: 15px;
            text-align: center;
        }

        canvas {
            border: 2px solid #667eea;
            box-shadow: 0 0 20px rgba(102, 126, 234, 0.3);
        }

        .metrics {
            margin-top: 15px;
            background: rgba(102, 126, 234, 0.1);
            padding: 15px;
            border-radius: 8px;
            width: 400px;
        }

        .metric {
            display: flex;
            justify-content: space-between;
            padding: 5px 0;
            font-size: 13px;
            border-bottom: 1px solid rgba(102, 126, 234, 0.2);
        }

        .metric:last-child {
            border-bottom: none;
        }

        .metric-value {
            color: #667eea;
            font-weight: bold;
        }

        .controls {
            background: rgba(102, 126, 234, 0.1);
            padding: 20px;
            border-radius: 8px;
            margin-bottom: 20px;
        }

        button {
            padding: 10px 20px;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            border: none;
            border-radius: 6px;
            color: white;
            font-weight: bold;
            cursor: pointer;
            margin: 0 5px;
            transition: transform 0.1s;
        }

        button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(102, 126, 234, 0.4);
        }

        .info-box {
            background: rgba(102, 126, 234, 0.1);
            padding: 20px;
            border-radius: 8px;
            max-width: 900px;
            font-size: 13px;
            line-height: 1.6;
        }

        .info-box strong {
            color: #667eea;
        }

        .highlight {
            color: #ff6b6b;
            font-weight: bold;
        }
    </style>
</head>
<body>
    <h1>üß™ Learning vs Static (No-Learning)</h1>
    <p class="subtitle">
        <strong>Key Finding from Research:</strong> Learning during simulation (backprop in-loop) produces dynamic, emergent behavior with cycles and oscillations.
        No-learning systems settle into boring, static steady states. Watch the difference unfold in real-time!
    </p>

    <div class="controls">
        <button id="btnReset">üîÑ Reset Both</button>
        <button id="btnPlayPause">‚è∏Ô∏è Pause</button>
        <span style="margin: 0 20px; color: #888;">Epoch: <span id="epochDisplay" style="color: #667eea; font-weight: bold;">0</span></span>
    </div>

    <div id="container">
        <div class="panel">
            <h2>üìö With Learning</h2>
            <p class="panel-subtitle">Learning Rate: 0.003 | Agents adapt continuously</p>
            <canvas id="canvasLearning" width="400" height="400"></canvas>
            <div class="metrics">
                <div class="metric">
                    <span>Entropy:</span>
                    <span class="metric-value" id="entropyLearning">0.00</span>
                </div>
                <div class="metric">
                    <span>Dominant Agent:</span>
                    <span class="metric-value" id="dominantLearning">-</span>
                </div>
                <div class="metric">
                    <span>Territory Changes:</span>
                    <span class="metric-value" id="changesLearning">0</span>
                </div>
            </div>
        </div>

        <div class="panel">
            <h2>üîí No Learning (Static)</h2>
            <p class="panel-subtitle">Learning Rate: 0 | Fixed network weights</p>
            <canvas id="canvasStatic" width="400" height="400"></canvas>
            <div class="metrics">
                <div class="metric">
                    <span>Entropy:</span>
                    <span class="metric-value" id="entropyStatic">0.00</span>
                </div>
                <div class="metric">
                    <span>Dominant Agent:</span>
                    <span class="metric-value" id="dominantStatic">-</span>
                </div>
                <div class="metric">
                    <span>Territory Changes:</span>
                    <span class="metric-value" id="changesStatic">0</span>
                </div>
            </div>
        </div>
    </div>

    <div class="info-box">
        <strong>What to Watch For:</strong><br><br>

        <strong>First 50-100 epochs:</strong> Both systems may look similar as they establish initial territories.<br><br>

        <strong>After 100+ epochs:</strong><br>
        ‚Ä¢ <span class="highlight">Learning system:</span> Continues to show dynamic patterns, territory swaps, and emergent cycles. Entropy stays high.<br>
        ‚Ä¢ <span class="highlight">Static system:</span> Quickly converges to a fixed pattern. One agent dominates and territories freeze. Entropy drops to near zero.<br><br>

        <strong>Paper Reference:</strong> Pages 9-10 show this divergence clearly. Learning enables open-ended exploration,
        while static weights lead to "information collapse" into trivial steady states.
    </div>

    <script>
        // Simplified PD-NCA for comparison demo
        const canvasL = document.getElementById('canvasLearning');
        const canvasS = document.getElementById('canvasStatic');
        const ctxL = canvasL.getContext('2d');
        const ctxS = canvasS.getContext('2d');

        const W = 64;
        const H = 64;
        const N = 3; // 3 agents
        const CELL_SIZE = canvasL.width / W;

        const COLORS = [
            [255, 100, 100],
            [100, 150, 255],
            [100, 255, 100],
            [80, 80, 80] // Environment
        ];

        let running = true;
        let epoch = 0;

        class SimpleNCA {
            constructor(withLearning) {
                this.withLearning = withLearning;
                this.learningRate = withLearning ? 0.003 : 0;

                // Simple state: just aliveness per agent
                this.aliveness = new Float32Array(W * H * (N + 1));
                this.prevDominant = new Int8Array(W * H);
                this.territoryChanges = 0;

                // Agent weights (simplified)
                this.weights = [];
                for (let i = 0; i <= N; i++) {
                    this.weights.push(new Float32Array(9).map(() => Math.random() * 0.2 - 0.1));
                }

                this.initialize();
            }

            initialize() {
                // Random initial territories
                for (let y = 0; y < H; y++) {
                    for (let x = 0; x < W; x++) {
                        const idx = (y * W + x) * (N + 1);
                        const primary = Math.floor(Math.random() * (N + 1));

                        for (let a = 0; a <= N; a++) {
                            this.aliveness[idx + a] = a === primary ? 0.8 : 0.2 / N;
                        }
                    }
                }
                this.normalize();
            }

            step() {
                const newAlive = new Float32Array(this.aliveness.length);

                for (let y = 0; y < H; y++) {
                    for (let x = 0; x < W; x++) {
                        const idx = (y * W + x) * (N + 1);

                        // Compute influence from neighbors for each agent
                        for (let agent = 0; agent <= N; agent++) {
                            let influence = 0;
                            let wIdx = 0;

                            for (let dy = -1; dy <= 1; dy++) {
                                for (let dx = -1; dx <= 1; dx++) {
                                    const nx = (x + dx + W) % W;
                                    const ny = (y + dy + H) % H;
                                    const nIdx = (ny * W + nx) * (N + 1) + agent;

                                    influence += this.aliveness[nIdx] * this.weights[agent][wIdx];
                                    wIdx++;
                                }
                            }

                            newAlive[idx + agent] = Math.max(0, Math.tanh(influence) * 0.5 + 0.5);
                        }

                        // Competition: boost max, suppress others
                        let maxVal = 0;
                        let maxAgent = 0;
                        for (let a = 0; a <= N; a++) {
                            if (newAlive[idx + a] > maxVal) {
                                maxVal = newAlive[idx + a];
                                maxAgent = a;
                            }
                        }

                        for (let a = 0; a <= N; a++) {
                            if (a === maxAgent) {
                                newAlive[idx + a] *= 1.1;
                            } else {
                                newAlive[idx + a] *= 0.8;
                            }
                        }
                    }
                }

                this.aliveness = newAlive;
                this.normalize();

                // Track territory changes
                this.territoryChanges = 0;
                for (let y = 0; y < H; y++) {
                    for (let x = 0; x < W; x++) {
                        const dominant = this.getDominant(x, y);
                        const prev = this.prevDominant[y * W + x];
                        if (dominant !== prev) {
                            this.territoryChanges++;
                        }
                        this.prevDominant[y * W + x] = dominant;
                    }
                }

                // Learning: adjust weights
                if (this.withLearning) {
                    this.learn();
                }
            }

            learn() {
                // Simplified learning: perturb weights based on territory control
                for (let agent = 0; agent < N; agent++) {
                    // Count territory
                    let territory = 0;
                    for (let i = 0; i < W * H; i++) {
                        const idx = i * (N + 1) + agent;
                        territory += this.aliveness[idx];
                    }

                    // Gradient (simplified): encourage expansion
                    const grad = (Math.random() * 2 - 1) * 0.01 * (1 - territory / (W * H));

                    for (let w = 0; w < this.weights[agent].length; w++) {
                        this.weights[agent][w] += this.learningRate * grad;
                        this.weights[agent][w] = Math.max(-1, Math.min(1, this.weights[agent][w]));
                    }
                }
            }

            normalize() {
                for (let i = 0; i < W * H; i++) {
                    const idx = i * (N + 1);
                    let sum = 0;
                    for (let a = 0; a <= N; a++) {
                        sum += this.aliveness[idx + a];
                    }
                    if (sum > 0) {
                        for (let a = 0; a <= N; a++) {
                            this.aliveness[idx + a] /= sum;
                        }
                    }
                }
            }

            getDominant(x, y) {
                const idx = (y * W + x) * (N + 1);
                let maxVal = 0;
                let maxAgent = N;
                for (let a = 0; a <= N; a++) {
                    if (this.aliveness[idx + a] > maxVal) {
                        maxVal = this.aliveness[idx + a];
                        maxAgent = a;
                    }
                }
                return maxAgent;
            }

            getEntropy() {
                let entropy = 0;
                for (let i = 0; i < this.aliveness.length; i++) {
                    const p = this.aliveness[i];
                    if (p > 0.01) {
                        entropy -= p * Math.log2(p);
                    }
                }
                return entropy / (W * H);
            }

            getDominantAgent() {
                const territory = new Array(N + 1).fill(0);
                for (let i = 0; i < W * H; i++) {
                    const idx = i * (N + 1);
                    for (let a = 0; a <= N; a++) {
                        territory[a] += this.aliveness[idx + a];
                    }
                }
                let maxIdx = 0;
                for (let i = 1; i <= N; i++) {
                    if (territory[i] > territory[maxIdx]) {
                        maxIdx = i;
                    }
                }
                return maxIdx < N ? `Agent ${maxIdx + 1}` : 'Environment';
            }

            render(ctx) {
                for (let y = 0; y < H; y++) {
                    for (let x = 0; x < W; x++) {
                        const dominant = this.getDominant(x, y);
                        const idx = (y * W + x) * (N + 1) + dominant;
                        const intensity = this.aliveness[idx];

                        const color = COLORS[dominant];
                        ctx.fillStyle = `rgb(${color[0] * intensity}, ${color[1] * intensity}, ${color[2] * intensity})`;
                        ctx.fillRect(x * CELL_SIZE, y * CELL_SIZE, CELL_SIZE, CELL_SIZE);
                    }
                }
            }
        }

        let ncaLearning = new SimpleNCA(true);
        let ncaStatic = new SimpleNCA(false);

        function animate() {
            if (running) {
                ncaLearning.step();
                ncaStatic.step();
                epoch++;

                // Update displays
                document.getElementById('epochDisplay').textContent = epoch;

                document.getElementById('entropyLearning').textContent = ncaLearning.getEntropy().toFixed(2);
                document.getElementById('dominantLearning').textContent = ncaLearning.getDominantAgent();
                document.getElementById('changesLearning').textContent = ncaLearning.territoryChanges;

                document.getElementById('entropyStatic').textContent = ncaStatic.getEntropy().toFixed(2);
                document.getElementById('dominantStatic').textContent = ncaStatic.getDominantAgent();
                document.getElementById('changesStatic').textContent = ncaStatic.territoryChanges;
            }

            ncaLearning.render(ctxL);
            ncaStatic.render(ctxS);

            requestAnimationFrame(animate);
        }

        document.getElementById('btnReset').addEventListener('click', () => {
            ncaLearning = new SimpleNCA(true);
            ncaStatic = new SimpleNCA(false);
            epoch = 0;
        });

        document.getElementById('btnPlayPause').addEventListener('click', function() {
            running = !running;
            this.textContent = running ? '‚è∏Ô∏è Pause' : '‚ñ∂Ô∏è Play';
        });

        animate();
    </script>
</body>
</html>
