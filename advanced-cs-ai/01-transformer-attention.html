<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Transformer Attention Head Visualizer - CCAB</title>
    <link rel="stylesheet" href="../assets/css/gallery-standard.css">
    <style>
        :root {
            --token-bg: #e2e8f0;
            --token-hover: #cbd5e1;
            --attention-color: rgba(99, 102, 241, 0.6);
        }

        body {
            background: #f8fafc;
            color: #1e293b;
            font-family: 'Segoe UI', system-ui, sans-serif;
            padding: 2rem;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
        }

        header {
            margin-bottom: 2rem;
            text-align: center;
        }

        h1 {
            color: #334155;
            margin-bottom: 0.5rem;
        }

        .controls {
            background: white;
            padding: 1.5rem;
            border-radius: 12px;
            box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);
            margin-bottom: 2rem;
            display: flex;
            gap: 1rem;
            align-items: center;
            flex-wrap: wrap;
        }

        textarea {
            flex: 1;
            padding: 0.75rem;
            border: 1px solid #cbd5e1;
            border-radius: 6px;
            font-family: monospace;
            min-height: 60px;
            resize: vertical;
        }

        button {
            background: #4f46e5;
            color: white;
            border: none;
            padding: 0.75rem 1.5rem;
            border-radius: 6px;
            cursor: pointer;
            font-weight: 600;
            transition: background 0.2s;
        }

        button:hover {
            background: #4338ca;
        }

        .visualization-area {
            display: grid;
            grid-template-columns: 1fr 300px;
            gap: 2rem;
            background: white;
            padding: 2rem;
            border-radius: 12px;
            box-shadow: 0 10px 15px -3px rgba(0, 0, 0, 0.1);
            min-height: 500px;
        }

        #attention-view {
            position: relative;
            padding: 2rem;
            border: 1px solid #e2e8f0;
            border-radius: 8px;
            overflow: auto;
        }

        .tokens-container {
            display: flex;
            flex-wrap: wrap;
            gap: 8px;
            line-height: 2.5;
        }

        .token {
            background: var(--token-bg);
            padding: 4px 8px;
            border-radius: 4px;
            cursor: pointer;
            position: relative;
            transition: background 0.2s, transform 0.1s;
            user-select: none;
        }

        .token:hover, .token.active {
            background: var(--token-hover);
            transform: translateY(-2px);
            z-index: 10;
        }

        .token.target {
            background: #fcd34d;
            border: 2px solid #f59e0b;
        }

        canvas {
            position: absolute;
            top: 0;
            left: 0;
            pointer-events: none;
            z-index: 5;
        }

        .sidebar {
            background: #f1f5f9;
            padding: 1.5rem;
            border-radius: 8px;
        }

        .head-selector {
            display: grid;
            grid-template-columns: repeat(4, 1fr);
            gap: 8px;
            margin-bottom: 1.5rem;
        }

        .head-btn {
            aspect-ratio: 1;
            border: 1px solid #cbd5e1;
            background: white;
            border-radius: 4px;
            cursor: pointer;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 0.8rem;
            color: #64748b;
        }

        .head-btn.active {
            background: #4f46e5;
            color: white;
            border-color: #4f46e5;
        }

        .info-panel h3 {
            margin-bottom: 0.5rem;
            color: #334155;
        }

        .info-panel p {
            font-size: 0.9rem;
            color: #64748b;
            line-height: 1.5;
        }
        
        /* Tooltip */
        .token-tooltip {
            position: absolute;
            background: #1e293b;
            color: white;
            padding: 4px 8px;
            border-radius: 4px;
            font-size: 0.75rem;
            pointer-events: none;
            display: none;
            z-index: 20;
            white-space: nowrap;
        }

    </style>
</head>
<body>

<div class="container">
    <a href="../index.html" class="gallery-back">‚Üê Back to Gallery</a>
    
    <header>
        <h1>Transformer Attention Visualizer</h1>
        <p class="subtitle">Explore how a "Mini-GPT" model attends to different parts of the input sequence.</p>
    </header>

    <div class="controls">
        <textarea id="input-text" placeholder="Type a sentence here...">The quick brown fox jumps over the lazy dog because it was feeling energetic.</textarea>
        <button id="process-btn">Process & Visualize</button>
    </div>

    <div class="visualization-area">
        <div id="attention-view">
            <div id="tokens-wrapper" class="tokens-container">
                <!-- Tokens will be injected here -->
            </div>
            <canvas id="connections-canvas"></canvas>
            <div id="tooltip" class="token-tooltip"></div>
        </div>

        <div class="sidebar">
            <div class="info-panel">
                <h3>Attention Heads (Layer 1)</h3>
                <p>Select a head to see its specific attention pattern. Different heads often learn different syntactic or semantic relationships.</p>
                <div class="head-selector" id="head-grid">
                    <!-- Head buttons -->
                </div>
            </div>
            
            <div class="info-panel" style="margin-top: 2rem;">
                <h3>Details</h3>
                <p id="details-text">Hover over a token to see which previous tokens it attends to. Brighter lines indicate stronger attention weights.</p>
            </div>
        </div>
    </div>
</div>

<script>
    // Mock "Mini-GPT" Simulation
    // In a real app, this would query a model via WebGPU or API. 
    // Here we procedurally generate plausible attention patterns.

    const inputText = document.getElementById('input-text');
    const processBtn = document.getElementById('process-btn');
    const tokensWrapper = document.getElementById('tokens-wrapper');
    const canvas = document.getElementById('connections-canvas');
    const ctx = canvas.getContext('2d');
    const headGrid = document.getElementById('head-grid');
    const tooltip = document.getElementById('tooltip');
    const detailsText = document.getElementById('details-text');

    let tokens = [];
    let attentionMatrix = []; // [Head][TargetToken][SourceToken]
    let currentHead = 0;
    let numHeads = 12;
    let activeTokenIndex = -1;

    // Initialize
    function init() {
        createHeadButtons();
        processText();
        
        window.addEventListener('resize', resizeCanvas);
        processBtn.addEventListener('click', processText);
        
        // Canvas interaction via token hover
        // Note: Event listeners are attached to tokens during generation
    }

    function createHeadButtons() {
        headGrid.innerHTML = '';
        for (let i = 0; i < numHeads; i++) {
            const btn = document.createElement('div');
            btn.className = `head-btn ${i === 0 ? 'active' : ''}`;
            btn.textContent = i + 1;
            btn.onclick = () => selectHead(i);
            headGrid.appendChild(btn);
        }
    }

    function selectHead(index) {
        currentHead = index;
        document.querySelectorAll('.head-btn').forEach((btn, i) => {
            btn.classList.toggle('active', i === index);
        });
        if (activeTokenIndex !== -1) {
            drawAttention(activeTokenIndex);
        }
        updateDetails();
    }

    function tokenize(text) {
        // Simple regex tokenizer for demo purposes
        return text.match(/\b[\w']+\b|[.,!?;]/g) || [];
    }

    function generateMockAttention(tokens) {
        // Generate pseudo-realistic attention patterns
        // Head 0: Previous token focus (local context)
        // Head 1: First token focus (global context)
        // Head 2: Random/Syntactic (e.g., attending to verbs)
        // Others: Mixed
        
        const matrix = [];
        
        for (let h = 0; h < numHeads; h++) {
            const headLayer = [];
            for (let t = 0; t < tokens.length; t++) {
                const tokenWeights = new Array(tokens.length).fill(0);
                
                // Causal mask: can only attend to past tokens (j <= t)
                for (let j = 0; j <= t; j++) {
                    let weight = 0;
                    
                    if (h === 0) {
                        // Focus on immediate neighbor
                        if (j === t - 1) weight = 0.8;
                        else if (j === t) weight = 0.2;
                    } else if (h === 1) {
                        // Focus on start of sentence
                        if (j === 0) weight = 0.7;
                        else weight = 0.3 / (t + 1);
                    } else if (h === 2) {
                        // Focus on "the" or nouns (simple heuristic)
                        if (['the', 'a', 'an'].includes(tokens[j].toLowerCase())) weight = 0.1;
                        else weight = Math.random();
                    } else {
                        // Distributed attention
                        weight = Math.random() * (j / (t + 1));
                    }
                    
                    tokenWeights[j] = weight;
                }
                
                // Softmax normalization
                const sum = tokenWeights.reduce((a, b) => a + b, 0);
                headLayer.push(tokenWeights.map(w => w / (sum || 1)));
            }
            matrix.push(headLayer);
        }
        return matrix;
    }

    function processText() {
        const text = inputText.value;
        tokens = tokenize(text);
        if (tokens.length === 0) return;

        attentionMatrix = generateMockAttention(tokens);
        renderTokens();
        resizeCanvas();
        activeTokenIndex = -1;
        ctx.clearRect(0, 0, canvas.width, canvas.height);
        updateDetails();
    }

    function renderTokens() {
        tokensWrapper.innerHTML = '';
        tokens.forEach((word, index) => {
            const el = document.createElement('span');
            el.className = 'token';
            el.textContent = word;
            el.dataset.index = index;
            
            el.addEventListener('mouseenter', (e) => {
                activeTokenIndex = index;
                drawAttention(index);
                highlightToken(el);
            });
            
            el.addEventListener('mouseleave', () => {
                // Optional: clear on leave or keep persistent?
                // Let's keep it until another is hovered for better UX
            });

            tokensWrapper.appendChild(el);
        });
    }

    function highlightToken(el) {
        document.querySelectorAll('.token').forEach(t => {
            t.classList.remove('target');
            t.style.backgroundColor = '';
        });
        el.classList.add('target');
    }

    function resizeCanvas() {
        const rect = document.getElementById('attention-view').getBoundingClientRect();
        // Adjust for scroll width/height if needed, but basic rect is fine for overlay
        canvas.width = rect.width;
        canvas.height = rect.height;
        
        // Redraw if active
        if (activeTokenIndex !== -1) {
            drawAttention(activeTokenIndex);
        }
    }

    function drawAttention(targetIndex) {
        ctx.clearRect(0, 0, canvas.width, canvas.height);
        
        const targetEl = tokensWrapper.children[targetIndex];
        if (!targetEl) return;
        
        const viewRect = document.getElementById('attention-view').getBoundingClientRect();
        const targetRect = targetEl.getBoundingClientRect();
        
        const startX = targetRect.left - viewRect.left + targetRect.width / 2;
        const startY = targetRect.top - viewRect.top;
        
        const weights = attentionMatrix[currentHead][targetIndex];
        
        // Sort indices by weight to draw strongest lines on top?
        // Actually, just loop through past tokens
        
        for (let i = 0; i <= targetIndex; i++) {
            const weight = weights[i];
            if (weight < 0.01) continue; // optimization
            
            const sourceEl = tokensWrapper.children[i];
            const sourceRect = sourceEl.getBoundingClientRect();
            
            const endX = sourceRect.left - viewRect.left + sourceRect.width / 2;
            const endY = sourceRect.bottom - viewRect.top;
            
            // Draw curve
            ctx.beginPath();
            ctx.moveTo(startX, startY);
            
            // Control points for nice arcs
            const cp1x = startX;
            const cp1y = startY - 30 - (weight * 50);
            const cp2x = endX;
            const cp2y = endY + 30 + (weight * 50);
            
            // Simpler quadratic curve for "over the top" visualization?
            // Let's do quadratic curves going UP from target and DOWN to source?
            // Actually, tokens are inline. We want arcs connecting them.
            // Since they wrap, simple lines might cross text. 
            // Let's use semi-circles/arcs above the text.
            
            const midX = (startX + endX) / 2;
            const dist = Math.abs(startX - endX);
            const arcHeight = Math.min(dist * 0.5, 150); 
            
            // If on same line vs different lines
            // Simple quadratic curve
            ctx.quadraticCurveTo(midX, Math.min(startY, endY) - arcHeight - 20, endX, startY + (targetRect.height/2)); 
            
            // Actually, better visualization for "Looking Back":
            // Draw lines from Top of Target to Bottom of Source? Or Top-to-Top.
            // Let's do Top-to-Top arcs.
            
            ctx.clearRect(0, 0, canvas.width, canvas.height); // Clear again to be safe inside loop? No.
        }

        // Re-loop for actual drawing to handle transparency correctly
        weights.forEach((weight, i) => {
            if (i > targetIndex || weight < 0.01) return;
            
            const sourceEl = tokensWrapper.children[i];
            const sourceRect = sourceEl.getBoundingClientRect();
            
            const endX = sourceRect.left - viewRect.left + sourceRect.width / 2;
            const endY = sourceRect.top - viewRect.top; // Top of token
            
            const sameToken = i === targetIndex;
            
            ctx.beginPath();
            if (sameToken) {
                // Self-attention loop
                ctx.arc(startX, startY, 10, 0, Math.PI * 2);
            } else {
                const dist = Math.abs(startX - endX);
                const controlY = startY - (dist * 0.3) - 20;
                ctx.moveTo(startX, startY);
                ctx.quadraticCurveTo((startX + endX)/2, controlY, endX, endY);
            }
            
            ctx.strokeStyle = `rgba(99, 102, 241, ${Math.min(weight * 2, 1)})`; // Amplify visibility
            ctx.lineWidth = Math.max(1, weight * 8);
            ctx.stroke();
            
            // Highlight source token background
            sourceEl.style.backgroundColor = `rgba(99, 102, 241, ${weight * 0.5})`;
        });
        
        // Keep target highlighted
        targetEl.style.backgroundColor = '#fcd34d';
    }

    function updateDetails() {
        const headType = [
            "Local Context (Previous Token)",
            "Global Anchor (First Token)",
            "Syntactic Relations",
            "Mixed / Distributed"
        ][currentHead % 4];
        
        detailsText.innerHTML = `<strong>Head ${currentHead + 1}:</strong> ${headType}<br>
        Visualizing attention distribution for the selected token. Thicker lines represent higher attention scores (softmax probability).`;
    }

    init();

</script>

</body>
</html>
